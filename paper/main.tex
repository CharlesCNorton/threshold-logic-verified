\documentclass[runningheads]{llncs}

\usepackage{amsmath,amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}

\lstset{
  language=Coq,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  breaklines=true,
  frame=single,
  xleftmargin=1em,
  xrightmargin=1em
}

\newcommand{\HW}{\mathrm{HW}}
\newcommand{\parity}{\mathrm{parity}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

\begin{document}

\title{Constructive Verification of a Threshold Network Computing Parity}

\author{Charles C. Norton}
\institute{Independent Researcher}

\maketitle

\begin{abstract}
We present a formally verified neural network that computes the parity function on 8-bit inputs. The network uses ternary weights in $\{-1, 0, +1\}$ and Heaviside activation functions. Correctness is established in the Coq proof assistant through a constructive algebraic proof that avoids exhaustive enumeration. The central insight is that for any $\pm 1$ weight vector $w$ and binary input $x$, the dot product $w \cdot x$ has the same parity as the Hamming weight of $x$. This property enables decomposition of the network's behavior into components whose parity-preserving structure can be verified symbolically. We generalize the result to arbitrary input size $n$, providing a verified construction of an $(n+3)$-neuron threshold circuit for $n$-bit parity. All proofs are machine-checkable and available as a public artifact.
\end{abstract}

\section{Introduction}

Neural network verification typically addresses local robustness: given a classifier and an input, prove that small perturbations do not change the output class. This paper addresses a different problem: prove that a neural network computes a specified mathematical function exactly on all inputs.

The function we verify is parity. Given $n$ binary inputs $x_0, \ldots, x_{n-1}$, the parity function returns 1 if an odd number of inputs are 1, and 0 otherwise. Equivalently, parity is the XOR of all input bits. For $n = 8$, there are 256 possible inputs, and the network must classify each correctly.

Parity is a canonical hard function for neural networks trained by gradient descent. The loss landscape is essentially flat because parity depends on a global property of the input (bit count modulo 2) rather than local features. Small weight perturbations produce negligible gradient signal until the network is already near a solution. This difficulty is well-documented in the machine learning literature and motivates our use of evolutionary search for training.

The network we verify is a threshold circuit with three layers. Each neuron computes a weighted sum of its inputs, adds a bias, and outputs 1 if the result is nonnegative, 0 otherwise. Weights are restricted to $\{-1, 0, +1\}$ (ternary quantization), and biases are bounded integers. This restriction makes the network amenable to exact formal analysis: all computations are in bounded integer arithmetic, and the behavior on any input is determined by sign comparisons.

The verification proceeds in two phases. First, we establish correctness for the specific 8-bit network found by evolutionary search. The initial proof uses Coq's \texttt{vm\_compute} tactic to exhaustively check all 256 inputs. This proof is valid but provides no insight into why the network computes parity.

Second, we develop a constructive proof that explains the network's behavior algebraically. The key theorem states that for any weight vector with entries in $\{-1, +1\}$ and any binary input vector of the same length, the dot product has the same parity as the Hamming weight. This allows us to characterize which neurons fire based on Hamming weight classes rather than individual inputs. Two of three layer-2 neurons fire on all inputs due to bias dominance. The third fires if and only if the Hamming weight is even. The output layer combines these to produce parity.

We then generalize beyond $n = 8$. The algebraic insight holds for arbitrary $n$, and we construct a verified threshold circuit using $n + 3$ neurons that computes $n$-bit parity. The construction uses a thermometer encoding of Hamming weight followed by an alternating-weight neuron that detects parity.

Finally, we establish depth bounds. A single threshold gate cannot compute parity for $n \geq 2$. A depth-2 circuit with $n$ neurons suffices. The trained network uses depth 3, which is not minimal but was found by undirected search.

All results are formalized in Coq 8.19. The development comprises approximately 4,000 lines across 13 files. Proofs compile in under one minute on commodity hardware. The artifact is publicly available.

\subsection{Contributions}

\begin{enumerate}
\item A trained ternary threshold network computing 8-bit parity, verified exhaustively and constructively in Coq.
\item The theorem that $\pm 1$ dot products preserve Hamming weight parity, enabling symbolic analysis of threshold networks.
\item A verified parametric construction of threshold circuits for $n$-bit parity using $n + 3$ neurons.
\item Proofs that depth 1 is insufficient and depth 2 is sufficient for parity.
\end{enumerate}

\section{Preliminaries}

\subsection{Threshold Gates and Networks}

A threshold gate with weights $w_1, \ldots, w_n \in \Z$ and bias $b \in \Z$ computes the function
\[
f(x_1, \ldots, x_n) = \begin{cases} 1 & \text{if } \sum_{i=1}^{n} w_i x_i + b \geq 0 \\ 0 & \text{otherwise} \end{cases}
\]
where each $x_i \in \{0, 1\}$. This is the Heaviside step function applied to an affine combination. The gate fires (outputs 1) when the weighted sum plus bias is nonnegative, and is silent (outputs 0) otherwise.

A threshold network arranges gates in layers. Layer 1 receives the input bits. Subsequent layers receive outputs from the previous layer. The final layer produces the network output. We consider feedforward networks where connections only go from layer $\ell$ to layer $\ell + 1$.

The depth of a network is the number of layers. The width is the maximum number of neurons in any layer. The size is the total number of neurons. Our trained 8-bit network has depth 3, width 32, and size 49 neurons.

Ternary quantization restricts weights to $\{-1, 0, +1\}$. This enables exact integer arithmetic throughout the network evaluation. The dot product of a ternary weight vector with a binary input vector is an integer in $[-n, +n]$ where $n$ is the dimension. With bounded integer biases, the pre-activation (weighted sum plus bias) is also bounded, and the sign comparison determining the gate output is exact.

\subsection{The Parity Function}

The parity function on $n$ bits is defined recursively:
\[
\parity([]) = 0, \qquad \parity([x_0, \ldots, x_{n-1}]) = x_0 \oplus \parity([x_1, \ldots, x_{n-1}])
\]
where $\oplus$ denotes XOR. Equivalently, $\parity(x) = \HW(x) \mod 2$, where $\HW(x) = \sum_i x_i$ is the Hamming weight (count of 1 bits).

Parity is a symmetric Boolean function: its value depends only on the number of 1s, not their positions. It is the simplest non-monotone symmetric function.

In circuit complexity, parity occupies a well-studied position. It lies in TC$^0$ (constant-depth, polynomial-size threshold circuits) but not in AC$^0$ (constant-depth circuits with AND/OR/NOT gates of unbounded fan-in). The separation of AC$^0$ and TC$^0$ is witnessed by parity.

\subsection{Coq Formalization}

Coq is a proof assistant based on the Calculus of Inductive Constructions. Definitions, theorems, and proofs are expressed in a typed functional language. The Coq kernel verifies that each proof term has the type corresponding to its theorem statement. A theorem is accepted only if its proof term type-checks.

We use standard Coq libraries for integers (\texttt{ZArith}), lists, and Booleans. The primary tactics are:
\begin{itemize}
\item \texttt{lia}: decides linear integer arithmetic
\item \texttt{destruct}: case analysis on inductive types
\item \texttt{induction}: structural induction on lists
\item \texttt{vm\_compute}: evaluates terms using a bytecode virtual machine
\end{itemize}

The \texttt{vm\_compute} tactic compiles goals to bytecode and executes them. For decidable properties on finite domains, it can check all cases directly. This is fast but provides no proof structure. A theorem proven by \texttt{vm\_compute} says only that the property was observed to hold after evaluation.

Constructive proofs instead derive the result through explicit logical steps. Each case in a case analysis generates a subgoal that must be discharged by tactics. The resulting proof term embeds the case structure and can be inspected or transformed.

\section{The Network}

\subsection{Architecture}

The network was found by evolutionary search over ternary weight configurations. Training evaluated candidate networks on all 256 possible 8-bit inputs, selected high-accuracy individuals, and applied mutation and recombination over thousands of generations until a network achieved 100\% accuracy.

The resulting architecture is $8 \to 32 \to 16 \to 1$:
\begin{itemize}
\item Input layer: 8 bits
\item Layer 1: 32 neurons, each with 8 ternary weights and an integer bias in $[-8, +8]$
\item Layer 2: 16 neurons, each with 32 ternary weights and an integer bias in $[-32, +32]$
\item Output: 1 neuron with 16 ternary weights and an integer bias in $[-16, +16]$
\end{itemize}

The total parameter count is $32 \times 9 + 16 \times 33 + 1 \times 17 = 833$. All weights are in $\{-1, 0, +1\}$, so the network contains no floating-point values.

\subsection{Neuron Ablation}

Systematic ablation analysis determined which neurons are critical for correct operation. For each neuron, we set its output to a constant (0 or 1) and evaluated accuracy on all 256 inputs. A neuron is critical if its ablation causes at least one misclassification.

Of the 32 layer-1 neurons, only 11 are critical. The remaining 21 can be removed without affecting accuracy. Of the 16 layer-2 neurons, only 3 are critical. The 13 redundant neurons suggest that evolutionary search found a solution with substantial structural slack.

The pruned network has architecture $8 \to 11 \to 3 \to 1$ with 139 parameters. It computes the same function as the full network and is the primary subject of our verification.

\subsection{Critical Neurons}

The 11 critical layer-1 neurons compute various threshold functions of the input bits. Each has a distinct $\pm 1/0$ weight pattern and integer bias. Some detect whether specific subsets of bits are mostly set. Others compute more complex predicates.

The 3 critical layer-2 neurons are:
\begin{itemize}
\item N0: bias $+30$, always fires (bias dominates any negative contribution from weights)
\item N1: bias $-3$, fires if and only if Hamming weight is even
\item N2: bias $+20$, always fires
\end{itemize}

The output neuron has weights $[+1, -1, +1]$ on the three layer-2 outputs and bias $-2$. Since N0 and N2 always output 1, the output pre-activation is $1 - \text{N1} + 1 - 2 = -\text{N1}$. The output fires when N1 is silent, which occurs when Hamming weight is odd. This is parity.

\subsection{Weight Export}

Trained weights were exported from PyTorch to JSON, then converted to Coq definitions by a Python script. The Coq file defines each weight matrix and bias vector as a constant:

\begin{lstlisting}
Definition w_l1n0 : list Z := [1; -1; 1; -1; 1; 1; 1; 1].
Definition b_l1n0 : Z := 0.
\end{lstlisting}

The full network definition composes these constants through layer functions that apply dot products, add biases, and threshold:

\begin{lstlisting}
Definition neuron (ws : list Z) (b : Z) (xs : list bool) : bool :=
  if Z.geb (dot ws xs + b) 0 then true else false.

Definition l1_out (x0 x1 x2 x3 x4 x5 x6 x7 : bool) : list bool :=
  let xs := [x0;x1;x2;x3;x4;x5;x6;x7] in
  [neuron w_l1n0 b_l1n0 xs; neuron w_l1n1 b_l1n1 xs; ...].
\end{lstlisting}

\section{Verification}

\subsection{Exhaustive Baseline}

The initial correctness proof enumerates all inputs:

\begin{lstlisting}
Theorem network_correct_exhaustive :
  forall xs, In xs (all_inputs 8) -> network xs = parity xs.
Proof. vm_compute. reflexivity. Qed.
\end{lstlisting}

The function \texttt{all\_inputs 8} generates the list of all $2^8 = 256$ bit vectors. The theorem states that for each such vector, the network output equals parity. The \texttt{vm\_compute} tactic evaluates both sides for all 256 cases, confirms they are equal, and the goal reduces to \texttt{true = true}.

This proof is valid but unsatisfying. It treats the theorem as a brute-force computation rather than a mathematical derivation. The proof term is opaque: it contains no information about why the equality holds. If we changed the network weights, the proof would either succeed or fail with no intermediate insight.

\subsection{Always-Fire Lemmas}

The first constructive observation is that some neurons fire regardless of input. A neuron with weights $w_1, \ldots, w_k$ and bias $b$ fires on all inputs if even the worst-case pre-activation is nonnegative.

Define the sum of negative weights:
\[
\text{sum\_neg}(w) = \sum_{i : w_i < 0} w_i
\]

If $\text{sum\_neg}(w) + b \geq 0$, the neuron fires on all inputs. This is because the pre-activation is $\sum_i w_i x_i + b$, and when weights are in $\{-1, 0, +1\}$, the minimum occurs when all negative-weight inputs are 1 and all positive-weight inputs are 0.

\begin{lstlisting}
Lemma neuron_always_fires : forall ws b xs,
  length ws = length xs ->
  sum_negative ws + b >= 0 ->
  neuron ws b xs = true.
\end{lstlisting}

Applying this to the pruned network:
\begin{itemize}
\item Layer-2 neuron N0 has $\text{sum\_neg} = -5$ and bias $+30$. Since $-5 + 30 = 25 \geq 0$, N0 always fires.
\item Layer-2 neuron N2 has $\text{sum\_neg} = -6$ and bias $+20$. Since $-6 + 20 = 14 \geq 0$, N2 always fires.
\end{itemize}

This eliminates two of three layer-2 neurons from further analysis. Only N1 requires input-dependent reasoning.

\subsection{The Parity Preservation Theorem}

The central algebraic insight concerns weight vectors with entries in $\{-1, +1\}$ (no zeros). For such a vector $w$ and binary input $x$ of the same length:

\begin{theorem}
$w \cdot x \equiv \HW(x) \pmod{2}$
\end{theorem}

\begin{proof}
Partition the positions into $P = \{i : w_i = +1\}$ and $N = \{i : w_i = -1\}$. Let $A = \sum_{i \in P} x_i$ and $B = \sum_{i \in N} x_i$. Then:
\begin{align*}
w \cdot x &= A - B \\
\HW(x) &= A + B
\end{align*}
Since $A - B = (A + B) - 2B = \HW(x) - 2B$, we have $w \cdot x \equiv \HW(x) \pmod{2}$.
\end{proof}

In Coq, we formalize this using auxiliary functions that count positive and negative contributions:

\begin{lstlisting}
Fixpoint count_pos (ws : list bool) (xs : list bool) : nat :=
  match ws, xs with
  | [], _ => 0
  | _, [] => 0
  | true :: ws', x :: xs' => (if x then 1 else 0) + count_pos ws' xs'
  | false :: ws', _ :: xs' => count_pos ws' xs'
  end.

Theorem dot_parity_equals_hw_parity : forall ws xs,
  length ws = length xs ->
  Z.even (signed_dot ws xs) = Nat.even (hamming_weight xs).
\end{lstlisting}

The proof proceeds by induction on the length of the lists, with arithmetic lemmas handling the even/odd transitions.

This theorem applies to any $\pm 1$ weight vector. Several layer-1 neurons in our network have such patterns (after treating 0-weight positions as inactive). Their pre-activations inherit parity properties from the input Hamming weight.

\subsection{ABC Decomposition}

The critical neuron N1 receives input from all 11 layer-1 neurons. Its pre-activation is:
\[
\text{pre}_{N1}(x) = \sum_{j=0}^{10} w^{(2)}_{1,j} \cdot h_j(x) + b^{(2)}_1
\]
where $h_j(x)$ is the output of layer-1 neuron $j$ and $w^{(2)}_1$ is the weight vector of N1.

We decompose this sum into three components based on the structure of the layer-1 neurons:

\begin{itemize}
\item $A(x)$: contribution from neurons whose behavior depends on a linear form $h(x) = c_0 x_0 + \cdots + c_7 x_7$ for some fixed coefficients
\item $B(x)$: contribution from neurons whose behavior depends only on Hamming weight
\item $C(x)$: contribution from neurons with $\pm 1$ weight patterns (parity-preserving)
\end{itemize}

The decomposition theorem:
\begin{lstlisting}
Theorem L2N1_decomposition : forall x0 x1 x2 x3 x4 x5 x6 x7,
  L2N1_preact x0 x1 x2 x3 x4 x5 x6 x7 =
  A x0 x1 x2 x3 x4 x5 x6 x7 +
  B x0 x1 x2 x3 x4 x5 x6 x7 +
  C x0 x1 x2 x3 x4 x5 x6 x7 - 3.
\end{lstlisting}

The component $C$ has a crucial property: it equals 0 whenever the Hamming weight is even. This follows from the parity preservation theorem applied to the $\pm 1$ weight patterns.

For even Hamming weight: $C = 0$, and analysis shows $A + B \geq 3$, so $\text{pre}_{N1} \geq 0$ and N1 fires.

For odd Hamming weight: $A + B + C \leq 2$, so $\text{pre}_{N1} < 0$ and N1 is silent.

\subsection{Constructive Proof}

The final proof assembles the structural lemmas:

\begin{lstlisting}
Theorem network_correct : forall x0 x1 x2 x3 x4 x5 x6 x7,
  network x0 x1 x2 x3 x4 x5 x6 x7 = parity [x0;x1;x2;x3;x4;x5;x6;x7].
Proof.
  intros.
  unfold network, layer2.
  rewrite L2N0_always_fires, L2N2_always_fires.
  rewrite L2N1_fires_iff_even.
  unfold output_neuron.
  destruct (Nat.even (hamming_weight [x0;x1;x2;x3;x4;x5;x6;x7]));
  reflexivity.
Qed.
\end{lstlisting}

The proof structure is:
\begin{enumerate}
\item Unfold the network definition to expose layer-2 neurons
\item Replace N0 and N2 with constant 1 (always-fire lemmas)
\item Replace N1 with the Hamming weight parity test (ABC decomposition result)
\item The output computation simplifies to $1 - (\text{even HW}) + 1 - 2 = -(\text{even HW})$
\item Case split on whether HW is even; both cases reduce to reflexivity
\end{enumerate}

This proof does not use \texttt{vm\_compute}. The 256-way case analysis occurs inside the lemma proofs (via \texttt{destruct} on the 8 input bits), but the main theorem applies the lemmas symbolically.

\section{Parametric Extension}

\subsection{Generalizing the Parity Theorem}

The parity preservation theorem holds for any length $n$, not just $n = 8$. The Coq formalization uses induction on lists:

\begin{lstlisting}
Theorem dot_parity_equals_hw_parity : forall ws xs,
  length ws = length xs ->
  Z.even (signed_dot ws xs) = Nat.even (hamming_weight xs).
Proof.
  intros ws xs Hlen.
  revert xs Hlen.
  induction ws as [| w ws' IH]; intros xs Hlen.
  - destruct xs; simpl in *; [reflexivity | discriminate].
  - destruct xs as [| x xs']; simpl in *; [discriminate |].
    injection Hlen as Hlen'.
    specialize (IH xs' Hlen').
    destruct w, x; simpl; rewrite IH;
    (* arithmetic cases handled by lia and even/odd lemmas *)
    ...
Qed.
\end{lstlisting}

The inductive step splits on whether the current weight is $+1$ or $-1$ and whether the current input is 0 or 1. Each case requires relating the parity of the updated dot product to the parity of the updated Hamming weight. Standard arithmetic lemmas about even and odd numbers complete the proof.

\subsection{Thermometer Encoding}

To construct a verified $n$-bit parity circuit, we use a thermometer encoding of the Hamming weight. Layer 1 has $n + 1$ neurons, each detecting whether at least $k$ bits are set for $k = 0, 1, \ldots, n$.

Neuron $k$ has all-ones weights and bias $-k$:
\[
h_k(x) = \begin{cases} 1 & \text{if } \HW(x) \geq k \\ 0 & \text{otherwise} \end{cases}
\]

The layer-1 output is a bit vector of the form $[1, 1, \ldots, 1, 0, 0, \ldots, 0]$ with $\HW(x) + 1$ ones followed by $n - \HW(x)$ zeros. This is a unary representation of the Hamming weight.

\begin{lstlisting}
Lemma L1_neuron_correct : forall k xs,
  k <= length xs ->
  neuron (repeat 1 (length xs)) (- Z.of_nat k) xs
    = (k <=? hamming_weight xs)%nat.
\end{lstlisting}

\subsection{Alternating Sum}

Layer 2 consists of a single neuron with alternating weights $[+1, -1, +1, -1, \ldots]$ and bias $-1$. On input $[1, 1, \ldots, 1, 0, \ldots, 0]$ with $m$ ones, the pre-activation is:
\[
\sum_{i=0}^{m-1} (-1)^i - 1 = \begin{cases} 0 & \text{if } m \text{ is even} \\ 1 - 1 = 0 & \text{if } m \text{ is odd} \end{cases}
\]

Wait, let us be precise. The alternating sum of the first $m$ terms of $[+1, -1, +1, -1, \ldots]$ is:
\[
\sum_{i=0}^{m-1} (-1)^i = \begin{cases} 1 & \text{if } m \text{ is odd} \\ 0 & \text{if } m \text{ is even} \end{cases}
\]

With bias $-1$, the neuron fires when the alternating sum $\geq 1$, i.e., when $m$ is odd. Since $m = \HW(x) + 1$, the neuron fires when $\HW(x)$ is even.

\begin{lstlisting}
Lemma alt_sum_first : forall m,
  alt_sum (repeat_alt m) = if Nat.odd m then 1 else 0.
\end{lstlisting}

\subsection{Output Layer}

The output neuron negates the layer-2 output: weight $-1$, bias $0$. It fires when layer 2 is silent, which occurs when $\HW(x)$ is odd. This is exactly parity.

\begin{lstlisting}
Theorem parity_network_correct : forall n xs,
  length xs = n ->
  parity_network n xs = parity xs.
\end{lstlisting}

The full construction uses $n + 3$ neurons: $n + 1$ in layer 1, 1 in layer 2, and 1 in the output. This is not asymptotically optimal (parity can be computed by depth-2 circuits with $O(n / \log n)$ gates), but the construction is simple and the proof is direct.

\subsection{Concrete vs Abstract}

The Coq development distinguishes between an abstract specification and a concrete implementation. The abstract network directly computes $\neg(\text{even}(\HW(x)))$:

\begin{lstlisting}
Definition parity_abstract (xs : list bool) : bool :=
  negb (Nat.even (hamming_weight xs)).
\end{lstlisting}

The concrete network uses actual threshold neurons:

\begin{lstlisting}
Definition parity_concrete (n : nat) (xs : list bool) : bool :=
  let h1 := map (fun k => neuron (ones n) (- Z.of_nat k) xs) (seq 0 (S n)) in
  let h2 := neuron (alt_weights (S n)) (-1) h1 in
  neuron [-1] 0 [h2].
\end{lstlisting}

The main theorem proves equivalence:

\begin{lstlisting}
Theorem concrete_eq_abstract : forall n xs,
  length xs = n ->
  parity_concrete n xs = parity_abstract xs.
\end{lstlisting}

Combined with the trivial fact that \texttt{parity\_abstract} equals \texttt{parity}, this establishes correctness of the concrete network.

\section{Depth Bounds}

\subsection{Depth-1 Impossibility}

A single threshold gate cannot compute parity for $n \geq 2$. The proof uses the four two-bit inputs as witnesses.

For any weights $w_0, w_1$ and bias $b$, parity requires:
\begin{align*}
b &< 0 & &\text{(input 00 $\to$ output 0)} \\
w_0 + b &\geq 0 & &\text{(input 10 $\to$ output 1)} \\
w_1 + b &\geq 0 & &\text{(input 01 $\to$ output 1)} \\
w_0 + w_1 + b &< 0 & &\text{(input 11 $\to$ output 0)}
\end{align*}

From the first three inequalities: $w_0 \geq -b > 0$ and $w_1 \geq -b > 0$. Thus $w_0 + w_1 \geq -2b$, so $w_0 + w_1 + b \geq -b > 0$. This contradicts the fourth inequality.

\begin{lstlisting}
Theorem depth1_impossible_2bit : forall w0 w1 b,
  threshold_gate [w0; w1] b [false; false] = false ->
  threshold_gate [w0; w1] b [true; false] = true ->
  threshold_gate [w0; w1] b [false; true] = true ->
  threshold_gate [w0; w1] b [true; true] = false ->
  False.
Proof.
  intros w0 w1 b H00 H10 H01 H11.
  (* convert gate outputs to arithmetic constraints *)
  rewrite gate_silent in H00, H11.
  rewrite gate_fires in H10, H01.
  (* simplify dot products *)
  simpl in *.
  (* derive contradiction *)
  lia.
Qed.
\end{lstlisting}

The generalization to $n$ bits pads the two-bit inputs with zeros. Since parity of a padded input equals parity of the original, the same constraints apply to the first two weights, yielding the same contradiction.

\begin{lstlisting}
Theorem depth1_impossible_nbit : forall n, (n >= 2)%nat ->
  forall ws b, length ws = n ->
  ~(forall xs, length xs = n -> threshold_gate ws b xs = parity xs).
\end{lstlisting}

\subsection{Depth-2 Sufficiency}

A depth-2 circuit computes $n$-bit parity using $n$ neurons in layer 1 and 1 neuron in layer 2. Layer 1 computes the ``at least $k$'' predicates for $k = 1, \ldots, n$. Layer 2 applies alternating weights.

For 2-bit parity, the explicit construction is:
\begin{itemize}
\item Neuron 1: weights $[1, 1]$, bias $-1$ (fires if $\HW \geq 1$)
\item Neuron 2: weights $[1, 1]$, bias $-2$ (fires if $\HW \geq 2$)
\item Output: weights $[1, -1]$, bias $-1$
\end{itemize}

\begin{lstlisting}
Theorem depth2_computes_parity_2bit :
  depth2_circuit [([1;1], -1); ([1;1], -2)] [1; -1] (-1) [false; false] = false /\
  depth2_circuit [([1;1], -1); ([1;1], -2)] [1; -1] (-1) [true; false] = true /\
  depth2_circuit [([1;1], -1); ([1;1], -2)] [1; -1] (-1) [false; true] = true /\
  depth2_circuit [([1;1], -1); ([1;1], -2)] [1; -1] (-1) [true; true] = false.
Proof. simpl. auto. Qed.
\end{lstlisting}

The trained network uses depth 3, which is one more than necessary. Evolutionary search did not optimize for depth, only for accuracy. A depth-2 network with 8 layer-1 neurons could compute 8-bit parity, but we did not train one.

\section{Related Work}

\subsection{Threshold Circuit Complexity}

The complexity-theoretic study of threshold circuits dates to the late 1980s. Parberry's textbook \cite{parberry1994} surveys the field. The key results relevant to parity:

\begin{itemize}
\item Parity is in TC$^0$: computable by constant-depth, polynomial-size threshold circuits \cite{siu1991}.
\item Depth-2 threshold circuits for parity require $\Omega(n / \log n)$ gates \cite{hajnal1993}.
\item Exponentially many depth-2 threshold circuits compute distinct Boolean functions \cite{muroga1971}.
\end{itemize}

Our work does not advance complexity theory. We provide machine-checked proofs of elementary facts and apply them to verify a specific trained network.

\subsection{Neural Network Verification}

The dominant paradigm in neural network verification addresses adversarial robustness: given a classifier and an input $x$, prove that all inputs within an $\epsilon$-ball of $x$ receive the same classification. Tools like Reluplex \cite{katz2017}, Marabou \cite{katz2019}, and $\alpha,\beta$-CROWN \cite{wang2021} encode the verification problem as satisfiability queries in theories combining linear arithmetic and piecewise-linear activations.

These tools target networks with hundreds to millions of neurons and continuous activations (ReLU, sigmoid). They provide sound but incomplete verification: failure to prove robustness may indicate a counterexample or may reflect solver limitations.

Our setting is different. The network is small (49 neurons), activations are discontinuous (Heaviside), and we prove exact functional correctness on all inputs rather than local robustness around one input. The discrete structure enables complete verification via proof assistants rather than SMT solvers.

\subsection{Verified Machine Learning}

Formal verification of machine learning systems has addressed various components:

\begin{itemize}
\item Training algorithms: proofs that gradient descent converges under certain conditions \cite{selsam2017}
\item Model architectures: type systems ensuring dimensional consistency \cite{vasilache2018}
\item Inference implementations: verified floating-point code for matrix multiplication \cite{boldo2015}
\end{itemize}

Functional correctness of a trained model (``this network computes function $f$'') is less explored. Most models compute approximations to unknown functions, so exact correctness is inapplicable. Our work applies to the special case where the target function is known and finite-domain.

\section{Conclusion}

We verified that a ternary threshold network computes 8-bit parity. The verification proceeds in Coq through constructive proofs that explain the network's algebraic structure. The key insight is that $\pm 1$ dot products preserve Hamming weight parity, enabling symbolic analysis of threshold gates without exhaustive enumeration.

The techniques extend to arbitrary input size. We constructed and verified an $(n+3)$-neuron threshold circuit for $n$-bit parity using thermometer encoding and alternating-weight detection. Depth bounds establish that one layer is insufficient and two layers suffice.

The primary limitation is scope. Parity is a symmetric Boolean function with rich algebraic structure. Extending to asymmetric functions or continuous-valued networks would require different techniques. The methodology applies when the target function is known, the domain is finite, and the network has discrete structure enabling exact analysis.

All Coq sources are available at \url{https://github.com/CharlesCNorton/threshold-logic-verified}. Trained weights are at \url{https://huggingface.co/phanerozoic/tiny-parity-prover}.

\bibliographystyle{splncs04}
\begin{thebibliography}{10}

\bibitem{boldo2015}
Boldo, S., Jourdan, J.H., Leroy, X., Melquiond, G.:
Verified compilation of floating-point computations.
Journal of Automated Reasoning 54(2), 135--163 (2015)

\bibitem{hajnal1993}
Hajnal, A., Maass, W., Pudl{\'a}k, P., Szegedy, M., Tur{\'a}n, G.:
Threshold circuits of bounded depth.
Journal of Computer and System Sciences 46(2), 129--154 (1993)

\bibitem{katz2017}
Katz, G., Barrett, C., Dill, D.L., Julian, K., Kochenderfer, M.J.:
Reluplex: An efficient SMT solver for verifying deep neural networks.
In: CAV. pp. 97--117 (2017)

\bibitem{katz2019}
Katz, G., Huang, D.A., Ibeling, D., Julian, K., Lazarus, C., Lim, R., Shah, P., Thakoor, S., Wu, H., Zelji{\'c}, A., et al.:
The Marabou framework for verification and analysis of deep neural networks.
In: CAV. pp. 443--452 (2019)

\bibitem{muroga1971}
Muroga, S.:
Threshold Logic and Its Applications.
Wiley-Interscience (1971)

\bibitem{parberry1994}
Parberry, I.:
Circuit Complexity and Neural Networks.
MIT Press (1994)

\bibitem{selsam2017}
Selsam, D., Liang, P., Dill, D.L.:
Developing bug-free machine learning systems with formal mathematics.
In: ICML. pp. 3047--3056 (2017)

\bibitem{siu1991}
Siu, K.Y., Roychowdhury, V., Kailath, T.:
Depth-efficient threshold circuits for arithmetic functions.
In: FOCS. pp. 578--587 (1991)

\bibitem{vasilache2018}
Vasilache, N., Zinenko, O., Theodoridis, T., Gober, P., Bastoul, C., Cohen, A.:
Tensor comprehensions: Framework-agnostic high-performance machine learning abstractions.
arXiv preprint arXiv:1802.04730 (2018)

\bibitem{wang2021}
Wang, S., Zhang, H., Xu, K., Lin, X., Jana, S., Hsieh, C.J., Kolter, J.Z.:
Beta-CROWN: Efficient bound propagation with per-neuron split constraints for neural network robustness verification.
In: NeurIPS. pp. 29909--29921 (2021)

\end{thebibliography}

\end{document}
